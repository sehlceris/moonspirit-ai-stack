# llama-swap configuration
# See: https://github.com/mostlygeek/llama-swap/wiki/Configuration

healthCheckTimeout: 600
logLevel: info
logToStdout: "proxy"
startPort: 10001
sendLoadingState: true

macros:
  "llama_server": "/opt/homebrew/bin/llama-server"
  "models_dir": "./models"

# Embeddings run in a separate group so they don't swap out the LLM.
groups:
  "llm":
    members: ["gpt-oss-120b", "glm-4.7-flash", "qwen3-coder-next"]
  "embeddings":
    members: ["nomic-embed"]

models:
  # --- LLMs (swap group — one loaded at a time) ---

  "gpt-oss-120b":
    cmd: >
      ${llama_server}
      --port ${PORT}
      --model ${models_dir}/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
      --ctx-size 8192
      --n-gpu-layers 999
      --flash-attn on
      --jinja
      --mlock
    ttl: 0
    aliases:
      - "gpt-oss"
      - "openai/gpt-oss-120b"

  "glm-4.7-flash":
    cmd: >
      ${llama_server}
      --port ${PORT}
      --model ${models_dir}/zai-org_GLM-4.7-Flash-GGUF/zai-org_GLM-4.7-Flash-Q4_K_M.gguf
      --ctx-size 8192
      --n-gpu-layers 999
      --flash-attn on
      --jinja
      --mlock
    ttl: 0
    aliases:
      - "glm-4.7"
      - "zai-org/GLM-4.7-Flash"

  "qwen3-coder-next":
    cmd: >
      ${llama_server}
      --port ${PORT}
      --model ${models_dir}/Qwen3-Coder-Next-GGUF/Qwen3-Coder-Next-Q4_K_M-00001-of-00004.gguf
      --ctx-size 32768
      --n-gpu-layers 999
      --flash-attn on
      --jinja
      --mlock
    ttl: 0
    aliases:
      - "qwen3-coder"
      - "Qwen/Qwen3-Coder-Next"

  # --- Embeddings (separate group — always available alongside LLM) ---

  "nomic-embed":
    cmd: >
      ${llama_server}
      --port ${PORT}
      --model ${models_dir}/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.f16.gguf
      --ctx-size 8192
      --n-gpu-layers 999
      --embedding
    ttl: 0
    aliases:
      - "nomic-embed-text"
      - "text-embedding-nomic"
      - "nomic-ai/nomic-embed-text-v1.5"
